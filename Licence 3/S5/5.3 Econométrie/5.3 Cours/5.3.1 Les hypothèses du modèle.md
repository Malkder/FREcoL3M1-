#### _Le modèle de régression linéaire_
Le modèle de régression prend la forme générale suivante :
$$\begin{align}
Y = f(X_1, X_2,\dots, X_{k})+\epsilon \\
Y= X_1\beta_{1}+X_{2}\beta_{3}+\dots+X_{K}\beta_{k}
\end{align}
$$
Avec :
- y la variable dépendante (ou expliquée)
- $x_{1},\dots,x_{k}$ les variables indépendantes (ou explicatives)
- $\beta$ un paramètre du modèle
- $\epsilon$ la perturbation
Ainsi, la valeur observée de $y_{i}$ s'écrit $y_{i} = x_{i1}\beta_{1}+x_{i2}+B_{2}+\dots+x_{ik}\beta_{k}+\epsilon_{i}$ : 
- Une composante déterministe
- Une composante aléatoire ($\epsilon_{i}$)
### Hypothèse du modèle de régression linéaire
#### _L'estimateur des moindres carrées ordinaires_
La méthode MCO consiste à retenir comme estimateur de $\alpha$ et $\beta$ la valeur qui minimise la somme des carrées des écarts entre la valeur observée et la variable calculée. (Somme des carrées résidus)

$$Min \sum^n_{t=1}\epsilon_{t}^2 =Min \sum_{t}(Y_{t}-\hat{Y_{t}})^2=Min\sum_{t}(Y_{t}-(\hat{\alpha}+\beta X_{t}))^2=Min \Phi(\hat{\alpha},\hat{\beta})$$La fonction phi est égale la somme des carrées résiduels.
Elle représente la distance au carré entre les valeurs théoriques et les valeurs expérimentales.
Nous poserons comme équation normal :
$$ 
\begin{align}
\frac{\partial \Phi}{\partial\hat{\alpha}}=0 \\
\frac{\partial \Phi}{\partial{\hat{\beta}}}=0
\end{align}
$$
Ainsi ;
$$\frac{\partial \Phi}{\partial\hat{\alpha}}=0 = -2\sum_{t}(Y_{t}-\hat{\alpha}-\hat{\beta}X_{t})=0 \bar{Y}n-\hat{\alpha}n-\hat{\beta}\bar{X}n=0$$
Ce qui conclue notre première équation normale par : $$\hat{\alpha}=\bar{Y}-\hat{\beta}\bar{X}$$
Pour la seconde équation normale ; 
$$\begin{align}
\frac{\partial \Phi}{\partial{\hat{\beta}}}=-2\sum_{t}(Y_{t}-\hat{\alpha}-\hat{\beta}X_{t})X_{t}=0 \\ \\
\equiv \sum_{t}Y_{t}X_{t}+\hat{\beta}\sum_{t}\bar{X}X_{t}-\bar{Y}\sum_{t}X_{t}-\hat{\beta}\sum_{t}X_{t}^2 = 0 \\
\equiv \sum_{t}Y_{t}X_{t}+\hat{\beta}\left( n\bar{X}\bar{X}-\sum_{t}X_{t}^2 \right)-\bar{Y}n\bar{X} =0 \\
\equiv \hat{\beta}=\sum\frac{X_{t}Y_{t}-n\bar{X}\bar{Y}}{\sum_{t}X_{t}^2-n\bar{X}^2} \\
\equiv \frac{\frac{1}{n}\sum_{t}X_{t}Y_{t}-\bar{X}\bar{Y}}{\frac{1}{n}\sum_{t}X_{t}^2-n\bar{X}^2}=\frac{Cov(X,Y)}{V(X)} & 
\end{align}$$
Ce qui conclue notre seconde équation normale par : [[démonstration]] 
$$\hat{\beta}=\frac{Cov(X,Y)}{V(X)}$$
On considère la fonction Phi comme défini positive pour gagner du temps, ainsi la fonction est convexe et nous trouvons bien un minimum.
[[Exemple dans le cas d'une absence de constante]]

#### _Les propriétés des estimateurs du MCO_
##### Hypothèses
###### Normalité des estimateurs
Sachant que $Y_{t}$ est une variable aléatoire car : 
$$Y_{t} = f(x_1, x_2,\dots, x_{k})+\epsilon$$
Alors
$$
\begin{align}
&x_t=X_t-\bar{X} \\
&\sum x_{t}=\sum(X_{t}-\bar{X}) = n\bar{X}-n\bar{X}=0 \\
&\hat{\beta}=\frac{\sum x_{t}Y_{t}}{\sum x_{t}^2} \\
&w_{t}=\frac{x_{t}}{\sum x_{t}^2}  \\
&\hat{\beta}=\sum w_{t}Y_{t} 
\end{align}
$$
Sachant que $w_{t}$ possèdent pour propriétés :
$$
\begin{align}
&\sum w_t =\frac{\sum x_{t}}{\sum x_{t}^2} \\
&\sum w_{t}^2 =\left( \frac{\sum x_{t}}{\sum x_{t}^2 } \right)^2 =\frac{1}{\sum x_{t}^2} \\
&\sum x_{t}w_{t} = \sum_{t}w_{t}(X_{t}-\bar{X})=\sum_{t}w_{t}X_{t}-\bar{X}\sum w_{t}=\sum w_{t}X_{t} \\ 
&\sum_{t}w_{t}x_{t}=\frac{\sum x_{t}x_{t}}{\sum x_{t}^2}=1
&\end{align}
$$
On peut par ailleurs ré-écrire :
$$ \hat{\alpha}=\bar{Y}-\hat{\beta}\bar{X} =\frac{1}{n}\sum Y_{t}-\bar{X}\sum w_{t}Y_{t} \equiv \sum(\frac{1}{n}-\bar{X}w_{t})Y_{t}$$
###### Estimateurs sans biais des paramètres du modèle
Commençons par $\alpha$ : 
$$ \begin{align}
&E[\hat{\alpha}] = E\left[ \sum\left( \frac{1}{n}-\bar{X}w_{t}\right) Y_{t}\right] \\
&= E\left[ \sum_{t}\left( \frac{1}{n}-\bar{X}w_{t}\right)(a+\beta X_{t}+\epsilon_{t}) \right] \\
&= E\left[  \sum\frac{1}{n}\alpha+\beta \sum \frac{X_{t}}{n}+\sum \frac{\epsilon_{t}}{n} - \bar{X}\sum w_{t}\alpha+\beta \bar{X}\sum w_{t}X_{t}-\bar{X}\sum w_{t}\epsilon_{t} \right] \\
&=E\left[ \alpha+\beta \bar{X}-\beta \bar{X}+\sum\left( \frac{1}{n}-\bar{X}w_{t}\right)\epsilon_{t} \right] \\
&\hat{\alpha}=\alpha+ \sum\left( \frac{1}{n}- \bar{X}w_{t}\right)\epsilon_{t} \\
&= \alpha+E\left[ \sum\left( \frac{1}{n}- \bar{X}w_{t} \right)E[\epsilon_{t}] \right] \\
&=E[\hat{\alpha}] = \alpha
\end{align}$$
.invi
